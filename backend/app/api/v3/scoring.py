"""
Scoring API - Integrates with existing AdvancedStrategyAnalysisEngine
ðŸ†• NEW FILE: Provides REST API access to existing strategic analysis services
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks, Query
import logging
from datetime import datetime, timezone
import json
from typing import Dict, Any, List, Optional

from ...core.database_config import db_manager

logger = logging.getLogger(__name__)

router = APIRouter(tags=["scoring"])

# âœ… REUSING EXISTING SERVICES (lazy load to avoid import errors)
analysis_engine = None
session_manager = None

def _init_services():
    """Lazy initialize services to handle missing dependencies"""
    global analysis_engine, session_manager
    
    if analysis_engine is None:
        try:
            from ...services.advanced_strategy_analysis import AdvancedStrategyAnalysisEngine
            analysis_engine = AdvancedStrategyAnalysisEngine()
        except Exception as e:
            logger.warning(f"AdvancedStrategyAnalysisEngine not available: {e}")
    
    if session_manager is None:
        try:
            from ...services.analysis_session_manager import AnalysisSessionManager
            session_manager = AnalysisSessionManager()
        except Exception as e:
            logger.debug(f"AnalysisSessionManager not available: {e}")

async def _execute_v2_analysis_background(session_id: str, topic_data: Dict, content_rows: List):
    """Execute v2.0 analysis in background without blocking HTTP response"""
    try:
        from ...services.v2_strategic_analysis_orchestrator import v2_orchestrator
        
        if not v2_orchestrator:
            logger.error(f"V2 orchestrator not available for background task {session_id}")
            return
        
        logger.info(f"ðŸš€ Background v2.0 analysis starting for {session_id}")
        
        # Execute complete analysis
        analysis_results = await v2_orchestrator.execute_complete_analysis(
            session_id=session_id,
            topic_knowledge=topic_data
        )
        
        logger.info(f"âœ… Background v2.0 analysis completed for {session_id}")
        logger.info(f"   Layers: {analysis_results.get('summary', {}).get('layers_analyzed', 0)}")
        logger.info(f"   Factors: {analysis_results.get('summary', {}).get('factors_calculated', 0)}")
        logger.info(f"   Segments: {analysis_results.get('summary', {}).get('segments_evaluated', 0)}")
        
    except Exception as e:
        logger.error(f"Background v2.0 analysis failed for {session_id}: {e}")
        # Fall back to saving mock results
        try:
            mock_results = await _create_mock_scoring(session_id, topic_data, content_rows)
            await _save_scoring_results(session_id, mock_results)
        except Exception as fallback_error:
            logger.error(f"Fallback mock scoring also failed: {fallback_error}")

async def _create_mock_scoring(session_id: str, topic_data: Dict, content_rows: List) -> Dict:
    """Create mock scoring results for demonstration purposes"""
    import random
    
    logger.info(f"Creating mock scoring results for {session_id}")
    
    # Calculate basic metrics from content
    total_words = sum(len(item['content'].split()) for item in topic_data['content_items'])
    avg_quality = sum(item['quality_score'] for item in topic_data['content_items']) / len(topic_data['content_items'])
    
    # Generate mock layer scores
    layers = {
        "MARKET_DYNAMICS": 0.65 + random.uniform(-0.1, 0.1),
        "COMPETITIVE_LANDSCAPE": 0.58 + random.uniform(-0.1, 0.1),
        "CONSUMER_BEHAVIOR": 0.72 + random.uniform(-0.1, 0.1),
        "PRODUCT_INNOVATION": 0.68 + random.uniform(-0.1, 0.1),
        "BRAND_POSITIONING": 0.61 + random.uniform(-0.1, 0.1),
        "OPERATIONAL_EXCELLENCE": 0.55 + random.uniform(-0.1, 0.1),
        "FINANCIAL_PERFORMANCE": 0.63 + random.uniform(-0.1, 0.1),
        "REGULATORY_ENVIRONMENT": 0.70 + random.uniform(-0.1, 0.1)
    }
    
    # Calculate factors from layers
    factors = {
        "Market_Attractiveness": (layers["MARKET_DYNAMICS"] * 0.4 + 
                                 layers["CONSUMER_BEHAVIOR"] * 0.3 + 
                                 layers["REGULATORY_ENVIRONMENT"] * 0.3),
        "Competitive_Strength": (layers["COMPETITIVE_LANDSCAPE"] * 0.3 + 
                                layers["PRODUCT_INNOVATION"] * 0.25 + 
                                layers["BRAND_POSITIONING"] * 0.25 + 
                                layers["OPERATIONAL_EXCELLENCE"] * 0.2),
        "Financial_Viability": (layers["FINANCIAL_PERFORMANCE"] * 0.6 + 
                               layers["OPERATIONAL_EXCELLENCE"] * 0.4),
        "Innovation_Potential": (layers["PRODUCT_INNOVATION"] * 0.5 + 
                                layers["MARKET_DYNAMICS"] * 0.3 + 
                                layers["CONSUMER_BEHAVIOR"] * 0.2)
    }
    
    # Generate segment scores
    segments = {
        "Premium_Market": {
            "attractiveness": factors["Market_Attractiveness"] * 0.7 + factors["Innovation_Potential"] * 0.3,
            "competitiveness": factors["Competitive_Strength"],
            "market_size": 0.6,
            "growth": 0.75
        },
        "Mass_Market": {
            "attractiveness": factors["Market_Attractiveness"] * 0.8 + factors["Financial_Viability"] * 0.2,
            "competitiveness": factors["Competitive_Strength"] * 0.9,
            "market_size": 0.9,
            "growth": 0.6
        },
        "Niche_Market": {
            "attractiveness": factors["Innovation_Potential"] * 0.6 + factors["Competitive_Strength"] * 0.4,
            "competitiveness": factors["Competitive_Strength"] * 0.7,
            "market_size": 0.4,
            "growth": 0.8
        }
    }
    
    # Calculate overall business case score
    business_case_score = (
        factors["Market_Attractiveness"] * 0.35 +
        factors["Competitive_Strength"] * 0.30 +
        factors["Financial_Viability"] * 0.20 +
        factors["Innovation_Potential"] * 0.15
    )
    
    return {
        "session_id": session_id,
        "business_case_score": round(business_case_score, 3),
        "layers": {name: round(score, 3) for name, score in layers.items()},
        "factors": {name: round(score, 3) for name, score in factors.items()},
        "segments": segments,
        "simulation_metadata": {
            "analysis_type": "mock",
            "content_items_analyzed": len(content_rows),
            "total_words": total_words,
            "average_content_quality": round(avg_quality, 3),
            "confidence_level": 0.7,
            "timestamp": datetime.now(timezone.utc).isoformat()
        },
        "scenarios": [
            {"name": "Base Case", "probability": 0.5, "score": business_case_score},
            {"name": "Optimistic", "probability": 0.3, "score": min(business_case_score + 0.15, 1.0)},
            {"name": "Pessimistic", "probability": 0.2, "score": max(business_case_score - 0.15, 0.0)}
        ]
    }

@router.get("/topics")
async def get_topics_for_scoring(user_id: Optional[str] = Query(None, description="User ID (optional)")):
    """
    Get all topics with their content and scoring readiness status
    ðŸ†• NEW ENDPOINT: Lists topics ready for strategic analysis
    """
    try:
        _init_services()  # Lazy load services
        connection = await db_manager.get_connection()
        
        # Get topics with content statistics - check scraped_content instead of topic_urls
        if user_id:
            query = """
            SELECT 
                t.session_id,
                t.topic,
                t.description,
                t.status,
                t.created_at,
                t.updated_at,
                t.metadata,
                COUNT(sc.id) as content_count,
                AVG(CAST(sc.metadata->>'quality_score' AS FLOAT)) as avg_quality,
                MAX(sc.scraped_at) as last_content_update
            FROM topics t
            LEFT JOIN scraped_content sc ON t.session_id = sc.session_id AND sc.processing_status = 'processed'
            WHERE t.user_id = $1
            GROUP BY t.session_id, t.topic, t.description, t.status, 
                     t.created_at, t.updated_at, t.metadata
            ORDER BY t.updated_at DESC
            """
            rows = await connection.fetch(query, user_id)
        else:
            # If no user_id provided, get all topics
            query = """
            SELECT 
                t.session_id,
                t.topic,
                t.description,
                t.status,
                t.created_at,
                t.updated_at,
                t.metadata,
                COUNT(sc.id) as content_count,
                AVG(CAST(sc.metadata->>'quality_score' AS FLOAT)) as avg_quality,
                MAX(sc.scraped_at) as last_content_update
            FROM topics t
            LEFT JOIN scraped_content sc ON t.session_id = sc.session_id AND sc.processing_status = 'processed'
            GROUP BY t.session_id, t.topic, t.description, t.status, 
                     t.created_at, t.updated_at, t.metadata
            ORDER BY t.updated_at DESC
            """
            rows = await connection.fetch(query)
        
        topics = []
        for row in rows:
            metadata = json.loads(row['metadata']) if row['metadata'] else {}
            
            # Determine scoring readiness based on scraped content
            content_count = row['content_count'] or 0
            has_content = content_count > 0
            avg_quality = float(row['avg_quality']) if row['avg_quality'] else 0.0
            
            # Check if already scored (prioritize v2.0 results)
            v2_score_query = """
            SELECT COUNT(*) as score_count, MAX(updated_at) as last_scored
            FROM v2_analysis_results
            WHERE session_id = $1
            """
            v2_score_row = await connection.fetchrow(v2_score_query, row['session_id'])
            
            # Fall back to basic scoring if no v2.0 results
            if (v2_score_row['score_count'] or 0) == 0:
                basic_score_query = """
                SELECT COUNT(*) as score_count, MAX(created_at) as last_scored
                FROM analysis_scores
                WHERE session_id = $1
                """
                score_row = await connection.fetchrow(basic_score_query, row['session_id'])
            else:
                score_row = v2_score_row
            
            has_scores = (score_row['score_count'] or 0) > 0
            last_scored = score_row['last_scored']
            
            # Determine scoring status
            if not has_content:
                scoring_status = "no_content"
            elif not has_scores:
                scoring_status = "never_scored"
            elif last_scored and row['last_content_update'] and row['last_content_update'] > last_scored:
                scoring_status = "needs_update"
            else:
                scoring_status = "up_to_date"
            
            topics.append({
                "session_id": row['session_id'],
                "topic": row['topic'],
                "description": row['description'],
                "status": row['status'],
                "created_at": row['created_at'].isoformat(),
                "updated_at": row['updated_at'].isoformat(),
                "content_statistics": {
                    "total_items": content_count,
                    "processed_items": content_count,
                    "average_quality": round(avg_quality, 3),
                    "has_content": has_content,
                    "last_content_update": row['last_content_update'].isoformat() if row['last_content_update'] else None
                },
                "scoring_information": {
                    "total_scores": score_row['score_count'] or 0,
                    "has_scores": has_scores,
                    "last_scored": last_scored.isoformat() if last_scored else None,
                    "scoring_status": scoring_status,
                    "ready_for_scoring": has_content
                },
                "metadata": metadata
            })
        
        return {"success": True, "topics": topics}
        
    except Exception as e:
        logger.error(f"Failed to get topics for scoring: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/{session_id}/start")
async def start_scoring(
    session_id: str,
    background_tasks: BackgroundTasks,
    user_id: Optional[str] = Query(None, description="User ID (optional)")
):
    """
    Start comprehensive strategic scoring workflow
    ðŸ†• NEW ENDPOINT: Executes existing AdvancedStrategyAnalysisEngine
    """
    try:
        connection = await db_manager.get_connection()
        
        # Get topic information (user_id optional)
        if user_id:
            topic_query = "SELECT * FROM topics WHERE session_id = $1 AND user_id = $2"
            topic_row = await connection.fetchrow(topic_query, session_id, user_id)
        else:
            topic_query = "SELECT * FROM topics WHERE session_id = $1"
            topic_row = await connection.fetchrow(topic_query, session_id)
        
        if not topic_row:
            raise HTTPException(status_code=404, detail="Topic not found")
        
        # Get scraped content for analysis
        content_query = """
        SELECT url, title, content, metadata
        FROM scraped_content
        WHERE session_id = $1 AND processing_status = 'processed'
        ORDER BY scraped_at DESC
        LIMIT 100
        """
        content_rows = await connection.fetch(content_query, session_id)
        
        if not content_rows:
            return {
                "success": False,
                "error": "No scraped content available for scoring. Please scrape content first.",
                "session_id": session_id
            }
        
        # Prepare data for analysis using scraped content
        topic_data = {
            "session_id": session_id,
            "topic": topic_row['topic'],
            "description": topic_row['description'],
            "content_items": [
                {
                    "url": row['url'],
                    "title": row['title'] or "Untitled",
                    "content": row['content'] or "",
                    "quality_score": float(json.loads(row['metadata']).get('quality_score', 0.0)) if row['metadata'] else 0.0,
                    "metadata": json.loads(row['metadata']) if row['metadata'] else {}
                }
                for row in content_rows
            ]
        }
        
        client_inputs = json.loads(topic_row['metadata']) if topic_row['metadata'] else {}
        
        # Run strategic analysis using v2.0 REAL LLM-based scoring
        logger.info(f"Starting v2.0 LLM-based strategic analysis for {session_id} with {len(content_rows)} content items")
        
        # Try to load v2.0 orchestrator
        v2_orchestrator = None
        try:
            from ...services.v2_strategic_analysis_orchestrator import v2_orchestrator as v2_orch
            v2_orchestrator = v2_orch
            logger.info("âœ… Using v2.0 orchestrator (210 layers, 28 factors, 5 segments)")
        except Exception as e:
            logger.warning(f"V2 orchestrator not available: {e}")
        
        # Use v2.0 real scoring if available - RUN IN BACKGROUND
        if v2_orchestrator:
            logger.info("ðŸš€ Starting v2.0 real LLM-based analysis in background...")
            
            # Run v2.0 analysis in background task (don't wait for completion)
            background_tasks.add_task(
                _execute_v2_analysis_background,
                session_id, topic_data, content_rows
            )
            
            # Return immediately so frontend doesn't timeout
            return {
                "success": True,
                "session_id": session_id,
                "scoring_started": True,
                "analysis_type": "v2.0_real_llm",
                "status": "in_progress",
                "message": "v2.0 Real LLM Analysis started in background (210 layers). Check results in 15-20 minutes.",
                "estimated_time_minutes": 15,
                "layers_to_analyze": 210,
                "factors_to_calculate": 28,
                "segments_to_evaluate": 5,
                "recommendation": "Come back in 15-20 minutes and click 'View Results' to see completed analysis"
            }
        else:
            # Fall back to mock scoring (synchronous)
            logger.warning(f"V2 orchestrator not available, using mock scoring for {session_id}")
            analysis_results = await _create_mock_scoring(session_id, topic_data, content_rows)
            
            # Save mock results
            await _save_scoring_results(session_id, analysis_results)
            
            return {
                "success": True,
                "session_id": session_id,
                "scoring_completed": True,
                "analysis_type": "mock",
                "results_summary": {
                    "business_case_score": analysis_results.get('business_case_score', 0.0),
                    "scenarios_generated": len(analysis_results.get('scenarios', [])),
                    "content_items_analyzed": len(content_rows),
                    "analysis_type": "mock"
                },
                "message": "Mock analysis completed (v2.0 orchestrator not available)"
            }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to start scoring for {session_id}: {e}")
        return {
            "success": False,
            "error": f"Scoring failed: {str(e)}",
            "session_id": session_id
        }

@router.get("/{session_id}/status")
async def get_scoring_status(session_id: str):
    """
    Get current status of scoring analysis
    Returns: 'not_started', 'in_progress', 'completed', 'failed'
    """
    try:
        connection = await db_manager.get_connection()
        
        # Check for v2.0 results
        v2_query = "SELECT created_at, layers_analyzed FROM v2_analysis_results WHERE session_id = $1"
        v2_row = await connection.fetchrow(v2_query, session_id)
        
        if v2_row:
            return {
                "session_id": session_id,
                "status": "completed",
                "analysis_type": "v2.0_real_llm",
                "layers_analyzed": v2_row['layers_analyzed'],
                "completed_at": v2_row['created_at'].isoformat(),
                "message": "v2.0 analysis completed"
            }
        
        # Check for mock results
        mock_query = "SELECT created_at FROM analysis_scores WHERE session_id = $1 ORDER BY created_at DESC LIMIT 1"
        mock_row = await connection.fetchrow(mock_query, session_id)
        
        if mock_row:
            return {
                "session_id": session_id,
                "status": "completed",
                "analysis_type": "mock",
                "completed_at": mock_row['created_at'].isoformat(),
                "message": "Mock analysis completed"
            }
        
        # Check if analysis is running (check recent logs or topic status)
        topic_query = "SELECT updated_at FROM topics WHERE session_id = $1"
        topic_row = await connection.fetchrow(topic_query, session_id)
        
        if topic_row:
            # Check if updated recently (within last 30 minutes - likely running)
            from datetime import datetime, timezone, timedelta
            if topic_row['updated_at'] > datetime.now(timezone.utc) - timedelta(minutes=30):
                return {
                    "session_id": session_id,
                    "status": "in_progress",
                    "message": "Analysis may be in progress. Check logs or wait for results."
                }
        
        return {
            "session_id": session_id,
            "status": "not_started",
            "message": "No analysis has been started for this session"
        }
        
    except Exception as e:
        logger.error(f"Failed to get scoring status for {session_id}: {e}")
        return {
            "session_id": session_id,
            "status": "unknown",
            "error": str(e)
        }

@router.get("/{session_id}/results")
async def get_scoring_results(session_id: str):
    """
    Get detailed scoring results for a topic
    Prioritizes v2.0 results if available, falls back to basic scoring
    """
    try:
        connection = await db_manager.get_connection()
        
        # First check for v2.0 results
        v2_query = """
        SELECT * FROM v2_analysis_results
        WHERE session_id = $1
        ORDER BY created_at DESC
        LIMIT 1
        """
        
        v2_row = await connection.fetchrow(v2_query, session_id)
        
        if v2_row:
            # Return v2.0 results
            logger.info(f"Returning v2.0 LLM-based results for {session_id}")
            
            # Parse full_results if it's a string (JSONB from database)
            full_results = v2_row['full_results']
            if isinstance(full_results, str):
                full_results = json.loads(full_results)
            
            return {
                "has_results": True,
                "session_id": session_id,
                "analysis_type": "v2.0_real_llm",
                "scored_at": v2_row['updated_at'].isoformat(),  # Use updated_at for latest analysis timestamp
                "results": {
                    "overall_score": float(v2_row['overall_business_case_score']),
                    "confidence": float(v2_row['overall_confidence']),
                    "business_case_score": float(v2_row['overall_business_case_score']),
                    "scenarios": full_results.get('scenarios', []),
                    "layer_scores": full_results.get('layer_scores', []),
                    "factor_scores": full_results.get('factor_calculations', []),
                    "segment_scores": full_results.get('segment_analyses', []),
                    "analysis_metadata": {
                        "layers_analyzed": v2_row['layers_analyzed'],
                        "factors_calculated": v2_row['factors_calculated'],
                        "segments_evaluated": v2_row['segments_evaluated'],
                        "processing_time": float(v2_row['processing_time_seconds']),
                        "analysis_type": "v2.0_real_llm",
                        "content_items": v2_row['content_items_analyzed']
                    }
                },
                "metadata": {
                    "version": "2.0",
                    "framework": "210 layers, 28 factors, 5 segments",
                    "llm_model": "Gemini 2.5 Pro"
                }
            }
        
        # Fall back to basic scoring results
        query = """
        SELECT 
            session_id, analysis_type, score, confidence,
            analysis_data, created_at, metadata
        FROM analysis_scores 
        WHERE session_id = $1 
        ORDER BY created_at DESC
        LIMIT 1
        """
        
        row = await connection.fetchrow(query, session_id)
        
        if not row:
            return {
                "has_results": False, 
                "message": "No scoring results found for this topic",
                "session_id": session_id
            }
        
        # Parse analysis data
        analysis_data = json.loads(row['analysis_data']) if row['analysis_data'] else {}
        metadata = json.loads(row['metadata']) if row['metadata'] else {}
        
        # Format basic scoring results for frontend
        logger.info(f"Returning basic/mock scoring results for {session_id}")
        return {
            "has_results": True,
            "session_id": session_id,
            "analysis_type": "mock",
            "scored_at": row['created_at'].isoformat(),
            "results": {
                "overall_score": row['score'],
                "confidence": row['confidence'],
                "business_case_score": analysis_data.get('business_case_score', row['score']),
                "scenarios": analysis_data.get('scenarios', []),
                "driver_sensitivities": analysis_data.get('driver_sensitivities', {}),
                "financial_projections": analysis_data.get('financial_projections', {}),
                "analysis_metadata": analysis_data.get('simulation_metadata', {}),
                "assumptions": analysis_data.get('assumptions', {}),
                # Add structured scores for frontend visualization
                "layer_scores": _extract_layer_scores(analysis_data),
                "factor_scores": _extract_factor_scores(analysis_data),
                "segment_scores": _extract_segment_scores(analysis_data)
            },
            "metadata": metadata
        }
        
    except Exception as e:
        logger.error(f"Failed to get scoring results for {session_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def _save_scoring_results(session_id: str, analysis_results: Dict[str, Any]):
    """
    Save scoring results to database
    ðŸ†• NEW HELPER: Stores analysis in existing schema
    """
    try:
        connection = await db_manager.get_connection()
        
        # Save to analysis_scores table (âœ… Using existing table)
        query = """
        INSERT INTO analysis_scores 
        (session_id, analysis_type, score, confidence, analysis_data, created_at, metadata)
        VALUES ($1, $2, $3, $4, $5, $6, $7)
        """
        
        await connection.execute(
            query,
            session_id,
            "comprehensive_strategic_analysis",
            analysis_results.get('business_case_score', 0.5),
            0.8,  # Default confidence
            json.dumps(analysis_results),
            datetime.now(timezone.utc),
            json.dumps({
                "analysis_completed_at": datetime.now(timezone.utc).isoformat(),
                "framework_version": "2.0"
            })
        )
        
        logger.info(f"âœ… Scoring results saved for {session_id}")
        
    except Exception as e:
        logger.error(f"Failed to save scoring results for {session_id}: {e}")
        raise

def _extract_layer_scores(analysis_data: Dict) -> List[Dict]:
    """
    Extract layer scores from analysis data
    ðŸ†• NEW HELPER: Transforms existing analysis format for frontend
    """
    # Try to extract from driver sensitivities or scenarios
    layer_scores = []
    
    sensitivities = analysis_data.get('driver_sensitivities', {})
    if sensitivities:
        for driver_name, sensitivity in list(sensitivities.items())[:8]:
            layer_scores.append({
                "layer_name": driver_name.upper().replace(' ', '_'),
                "score": min(1.0, abs(float(sensitivity)) / 100),  # Normalize
                "confidence": 0.75,
                "evidence_count": len(analysis_data.get('scenarios', [])),
                "insights": [f"Key driver with {abs(float(sensitivity)):.1f}% impact"]
            })
    
    # If no sensitivities, create default layers from scenarios
    if not layer_scores and analysis_data.get('scenarios'):
        default_layers = [
            "MARKET_DYNAMICS", "COMPETITIVE_LANDSCAPE", "CONSUMER_BEHAVIOR",
            "PRODUCT_INNOVATION", "BRAND_POSITIONING", "FINANCIAL_PERFORMANCE"
        ]
        scenarios = analysis_data.get('scenarios', [])
        for layer in default_layers[:6]:
            layer_scores.append({
                "layer_name": layer,
                "score": 0.65 if scenarios else 0.5,
                "confidence": 0.7,
                "evidence_count": len(scenarios),
                "insights": [f"Analysis available for {layer.replace('_', ' ').title()}"]
            })
    
    return layer_scores

def _extract_factor_scores(analysis_data: Dict) -> List[Dict]:
    """
    Extract factor scores from analysis data
    ðŸ†• NEW HELPER: Transforms existing analysis format for frontend
    """
    factor_scores = []
    
    # Extract from business case score
    business_score = analysis_data.get('business_case_score', 0.5)
    
    factors = [
        ("Market_Attractiveness", business_score * 0.9),
        ("Competitive_Strength", business_score * 0.85),
        ("Financial_Viability", business_score * 0.95),
        ("Innovation_Potential", business_score * 0.8)
    ]
    
    for factor_name, value in factors:
        factor_scores.append({
            "factor_name": factor_name,
            "value": value,
            "confidence": 0.75,
            "input_layers": ["MARKET_DYNAMICS", "COMPETITIVE_LANDSCAPE"],
            "formula": f"Derived from business case analysis"
        })
    
    return factor_scores

def _extract_segment_scores(analysis_data: Dict) -> List[Dict]:
    """
    Extract segment scores from analysis data
    ðŸ†• NEW HELPER: Transforms existing analysis format for frontend
    """
    segment_scores = []
    
    # Use scenarios to infer segment scores
    scenarios = analysis_data.get('scenarios', [])
    business_score = analysis_data.get('business_case_score', 0.5)
    
    segments = [
        {
            "segment_name": "Premium_Market",
            "attractiveness": business_score * 0.9,
            "competitiveness": business_score * 0.85,
            "market_size": business_score * 0.7,
            "growth": business_score * 0.95
        },
        {
            "segment_name": "Mass_Market",
            "attractiveness": business_score * 0.85,
            "competitiveness": business_score * 0.9,
            "market_size": business_score * 0.95,
            "growth": business_score * 0.8
        },
        {
            "segment_name": "Niche_Market",
            "attractiveness": business_score * 0.8,
            "competitiveness": business_score * 0.7,
            "market_size": business_score * 0.5,
            "growth": business_score * 0.9
        }
    ]
    
    return segments

@router.get("/{session_id}/quick-score")
async def get_quick_score(session_id: str):
    """
    Get quick scoring summary without full analysis
    ðŸ†• NEW ENDPOINT: Fast scoring overview
    """
    try:
        connection = await db_manager.get_connection()
        
        # Calculate quick score from URL quality metrics
        query = """
        SELECT 
            COUNT(*) as total_urls,
            AVG(quality_score) as avg_quality,
            AVG(relevance_score) as avg_relevance,
            COUNT(CASE WHEN quality_score >= 0.7 THEN 1 END) as high_quality_count
        FROM topic_urls
        WHERE session_id = $1
        """
        
        row = await connection.fetchrow(query, session_id)
        
        if not row or not row['total_urls']:
            return {
                "has_score": False,
                "message": "No data available for scoring"
            }
        
        # Calculate quick score
        avg_quality = float(row['avg_quality']) if row['avg_quality'] else 0.5
        avg_relevance = float(row['avg_relevance']) if row['avg_relevance'] else 0.5
        high_quality_ratio = (row['high_quality_count'] or 0) / (row['total_urls'] or 1)
        
        quick_score = (avg_quality * 0.4 + avg_relevance * 0.3 + high_quality_ratio * 0.3)
        
        return {
            "has_score": True,
            "session_id": session_id,
            "quick_score": round(quick_score, 3),
            "metrics": {
                "average_quality": round(avg_quality, 3),
                "average_relevance": round(avg_relevance, 3),
                "high_quality_ratio": round(high_quality_ratio, 3),
                "total_urls": row['total_urls']
            },
            "recommendation": _get_recommendation(quick_score, row['total_urls'])
        }
        
    except Exception as e:
        logger.error(f"Failed to get quick score for {session_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def _get_recommendation(score: float, url_count: int) -> str:
    """Generate recommendation based on score"""
    if score >= 0.7 and url_count >= 20:
        return "Excellent - Ready for full strategic analysis"
    elif score >= 0.5 and url_count >= 10:
        return "Good - Proceed with analysis"
    elif url_count < 10:
        return "Collect more URLs for better analysis"
    else:
        return "Consider improving URL quality before full analysis"
